{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 重新默写GPT架构"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "776fa06baef128bc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import  nn\n",
    "from torch.nn import functional as F\n",
    "import math"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-20T02:41:33.899631Z",
     "start_time": "2024-06-20T02:41:33.875755Z"
    }
   },
   "id": "f1a901ea74f3eb06",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 模型参数设置位置！\n",
    "class ModelArgs:\n",
    "    def __init__(self):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.block_size = 128 # 窗口大小GPT2为1024\n",
    "        self.batch_size = 32 # 暂定，之后再看显存占用\n",
    "        self.n_layer = 3\n",
    "        self.vocab_size = 7000\n",
    "        self.n_head = 6\n",
    "        self.n_embed = 768\n",
    "        self.bias = False\n",
    "        self.dropout = 0.0\n",
    "        # self.dataset_path = './data/sherlock'\n",
    "        # self.init_from = 'scratch'# 'scratch' or 'resume' # 从头训练还是继续\n",
    "        # self.checkpoint_save_dir = ''\n",
    "        self.eval_step = 50 # 每n步eval和保存checkpoint一次\n",
    "        self.flash_attn = False\n",
    "        # 学习率衰减\n",
    "        self.learning_rate = 6e-4\n",
    "        # self.warmup_iters = 2000\n",
    "        # self.lr_decay_iters = 8000\n",
    "        # self.min_lr = 6e-5\n",
    "        # 优化器参数\n",
    "        self.max_epochs = 10 # 训练多少个epoch\n",
    "        # self.weight_decay = 1e-1\n",
    "        # self.betas = (0.9,0.95)\n",
    "        # self.grad_clip = 1.0 # 梯度裁剪\n",
    "args= ModelArgs()\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.qkv = nn.Linear(args.n_embed, 3*args.n_embed, bias=args.bias)\n",
    "        self.dropout = args.dropout\n",
    "        self.dropout_attn = nn.Dropout(args.dropout)\n",
    "        self.n_embed = args.n_embed\n",
    "        self.n_head = args.n_head\n",
    "        assert self.n_embed % self.n_head == 0\n",
    "        self.head_dim = self.n_embed // self.n_head\n",
    "        self.flash_attn = args.flash_attn\n",
    "        self.attn_proj = nn.Linear(args.n_embed, args.n_embed, bias=args.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        q, k, v = self.qkv(x).split(self.n_embed, dim=-1)\n",
    "        q = q.reshape(B, T, self.n_head, self.head_dim).permute(0,2,1,3)\n",
    "        k = k.reshape(B, T, self.n_head, self.head_dim).permute(0,2,1,3)\n",
    "        v = v.reshape(B, T, self.n_head, self.head_dim).permute(0,2,1,3)\n",
    "        \n",
    "        if self.flash_attn:\n",
    "            attn = F.scaled_dot_product_attention(q, k, v, attn_mask=None,\n",
    "                                                  dropout_p=self.dropout if self.training else 0,\n",
    "                                                  is_causal=True)\n",
    "        else:\n",
    "            score = q @ k.permute(0,1,3,2)\n",
    "            score = score / (math.sqrt(self.head_dim))\n",
    "            mask = torch.tril(torch.ones(T,T,device=x.device)).reshape(1,1,T,T) == 0\n",
    "            score = score.masked_fill(mask, float('-inf'))\n",
    "            score = F.softmax(score, dim=-1)\n",
    "            if self.training:\n",
    "                score = self.dropout_attn(score)\n",
    "            attn = score @ v\n",
    "        \n",
    "        attn = attn.permute(0,2,1,3).reshape(B,T,C)\n",
    "        \n",
    "        return self.dropout_attn(self.attn_proj(attn))\n",
    "    \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super().__init__()\n",
    "        self.up_proj = nn.Linear(args.n_embed, 4*args.n_embed, bias=args.bias)\n",
    "        self.down_proj = nn.Linear(4*args.n_embed, args.n_embed, bias=args.bias)\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.down_proj(self.relu(self.up_proj(x))))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super().__init__()\n",
    "        self.attn = Attention(args)\n",
    "        self.mlp = MLP(args)\n",
    "        self.norm =  nn.LayerNorm(args.n_embed)\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm(x))\n",
    "        return x + self.mlp(self.norm(x))\n",
    "    \n",
    "class GPT(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(args.vocab_size, args.n_embed),\n",
    "            wpe = nn.Embedding(args.block_size, args.n_embed),\n",
    "            drop=nn.Dropout(args.dropout),\n",
    "            h = nn.ModuleList([Block(args) for _ in range(args.n_layer)]),\n",
    "            norm = nn.LayerNorm(args.n_embed)\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(args.n_embed, args.vocab_size, bias=False)\n",
    "        self.param_nums = 0\n",
    "        # 初始化\n",
    "        self.lm_head.weight = self.transformer.wte.weight\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        for pname, p in self.named_parameters():\n",
    "            self.param_nums += p.numel()\n",
    "            if pname.endswith('attn_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0, std=0.02/math.sqrt(1*args.n_layer))\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module ,nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0,std=0.02)\n",
    "            if module.bias is not None:\n",
    "                 torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0,std=0.02)\n",
    "            \n",
    "    def forward(self, idx, target=None):\n",
    "        B,T = idx.shape\n",
    "        device = idx.device\n",
    "        pos = torch.arange(0,T,dtype=torch.long,device=device)\n",
    "        \n",
    "        embed_wte = self.transformer.wte(idx)\n",
    "        embed_wpe = self.transformer.wpe(pos)\n",
    "        x = self.transformer.dropout(embed_wte + embed_wpe)\n",
    "        \n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.norm(x)\n",
    "        \n",
    "        if target is not None:\n",
    "            logits = self.lm_head(x) # logits [B,T,vocab_size]\n",
    "            loss = F.cross_entropy(logits.reshape(-1,logits.shape[-1]), target.reshape(-1), ignore_index=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(x) # logits [B,T,vocab_size]\n",
    "            loss = None\n",
    "        \n",
    "        for name, p in model.named_parameters():\n",
    "            print('*'*100)\n",
    "            print(name, '/', p.shape)\n",
    "            print(p.requires_grad ,'/', p.grad)\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    \n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx = idx if idx.shape[-1] < self.args.block_size else idx[:, -self.args.block_size:] # 截断取最靠后的idx\n",
    "            logits, _ = self(idx) # logits[b,t,vocab_size]\n",
    "            logits = logits[:, -1, :] / temperature # logits[b,vocab_size]\n",
    "            \n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, k=top_k) # v [b,top_k]\n",
    "                logits[logits < v[:, [-1]]] = float('-inf')\n",
    "                \n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, next_token], dim=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-20T05:12:22.517796Z",
     "start_time": "2024-06-20T05:12:22.422742Z"
    }
   },
   "id": "ea19b8e2bf832096",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = GPT(args)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-20T05:16:34.310430Z",
     "start_time": "2024-06-20T05:16:34.072045Z"
    }
   },
   "id": "4cd3605bcad60fdd",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "transformer.wte.weight / torch.Size([7000, 768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.wpe.weight / torch.Size([128, 768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.0.attn.qkv.weight / torch.Size([2304, 768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.0.attn.attn_proj.weight / torch.Size([768, 768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.0.mlp.up_proj.weight / torch.Size([3072, 768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.0.mlp.down_proj.weight / torch.Size([768, 3072])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.0.norm.weight / torch.Size([768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.0.norm.bias / torch.Size([768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.1.attn.qkv.weight / torch.Size([2304, 768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.1.attn.attn_proj.weight / torch.Size([768, 768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.1.mlp.up_proj.weight / torch.Size([3072, 768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.1.mlp.down_proj.weight / torch.Size([768, 3072])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.1.norm.weight / torch.Size([768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.1.norm.bias / torch.Size([768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.2.attn.qkv.weight / torch.Size([2304, 768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.2.attn.attn_proj.weight / torch.Size([768, 768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.2.mlp.up_proj.weight / torch.Size([3072, 768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.2.mlp.down_proj.weight / torch.Size([768, 3072])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.2.norm.weight / torch.Size([768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.2.norm.bias / torch.Size([768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.norm.weight / torch.Size([768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.norm.bias / torch.Size([768])\n",
      "True / None\n"
     ]
    }
   ],
   "source": [
    "for name,p in model.named_parameters():\n",
    "    print('*'*100)\n",
    "    print(name, '/', p.shape)\n",
    "    print(p.requires_grad ,'/', p.grad)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-20T05:22:29.451719Z",
     "start_time": "2024-06-20T05:22:29.438450Z"
    }
   },
   "id": "ec15c099194b86d1",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "63161100ed647433"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
