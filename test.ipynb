{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 重新默写GPT架构"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "776fa06baef128bc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import  nn\n",
    "from torch.nn import functional as F\n",
    "import math"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T06:38:01.232421Z",
     "start_time": "2024-06-21T06:37:59.933139Z"
    }
   },
   "id": "f1a901ea74f3eb06",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 模型参数设置位置！\n",
    "class ModelArgs:\n",
    "    def __init__(self):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.block_size = 128 # 窗口大小GPT2为1024\n",
    "        self.batch_size = 32 # 暂定，之后再看显存占用\n",
    "        self.n_layer = 3\n",
    "        self.vocab_size = 7000\n",
    "        self.n_head = 6\n",
    "        self.n_embed = 768\n",
    "        self.bias = False\n",
    "        self.dropout = 0.0\n",
    "        # self.dataset_path = './data/sherlock'\n",
    "        # self.init_from = 'scratch'# 'scratch' or 'resume' # 从头训练还是继续\n",
    "        # self.checkpoint_save_dir = ''\n",
    "        self.eval_step = 50 # 每n步eval和保存checkpoint一次\n",
    "        self.flash_attn = False\n",
    "        # 学习率衰减\n",
    "        self.learning_rate = 6e-4\n",
    "        # self.warmup_iters = 2000\n",
    "        # self.lr_decay_iters = 8000\n",
    "        # self.min_lr = 6e-5\n",
    "        # 优化器参数\n",
    "        self.max_epochs = 10 # 训练多少个epoch\n",
    "        # self.weight_decay = 1e-1\n",
    "        # self.betas = (0.9,0.95)\n",
    "        # self.grad_clip = 1.0 # 梯度裁剪\n",
    "args= ModelArgs()\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.qkv = nn.Linear(args.n_embed, 3*args.n_embed, bias=args.bias)\n",
    "        self.dropout = args.dropout\n",
    "        self.dropout_attn = nn.Dropout(args.dropout)\n",
    "        self.n_embed = args.n_embed\n",
    "        self.n_head = args.n_head\n",
    "        assert self.n_embed % self.n_head == 0\n",
    "        self.head_dim = self.n_embed // self.n_head\n",
    "        self.flash_attn = args.flash_attn\n",
    "        self.attn_proj = nn.Linear(args.n_embed, args.n_embed, bias=args.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        q, k, v = self.qkv(x).split(self.n_embed, dim=-1)\n",
    "        q = q.reshape(B, T, self.n_head, self.head_dim).permute(0,2,1,3)\n",
    "        k = k.reshape(B, T, self.n_head, self.head_dim).permute(0,2,1,3)\n",
    "        v = v.reshape(B, T, self.n_head, self.head_dim).permute(0,2,1,3)\n",
    "        \n",
    "        if self.flash_attn:\n",
    "            attn = F.scaled_dot_product_attention(q, k, v, attn_mask=None,\n",
    "                                                  dropout_p=self.dropout if self.training else 0,\n",
    "                                                  is_causal=True)\n",
    "        else:\n",
    "            score = q @ k.permute(0,1,3,2)\n",
    "            score = score / (math.sqrt(self.head_dim))\n",
    "            mask = torch.tril(torch.ones(T,T,device=x.device)).reshape(1,1,T,T) == 0\n",
    "            score = score.masked_fill(mask, float('-inf'))\n",
    "            score = F.softmax(score, dim=-1)\n",
    "            if self.training:\n",
    "                score = self.dropout_attn(score)\n",
    "            attn = score @ v\n",
    "        \n",
    "        attn = attn.permute(0,2,1,3).reshape(B,T,C)\n",
    "        \n",
    "        return self.dropout_attn(self.attn_proj(attn))\n",
    "    \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super().__init__()\n",
    "        self.up_proj = nn.Linear(args.n_embed, 4*args.n_embed, bias=args.bias)\n",
    "        self.down_proj = nn.Linear(4*args.n_embed, args.n_embed, bias=args.bias)\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.down_proj(self.relu(self.up_proj(x))))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super().__init__()\n",
    "        self.attn = Attention(args)\n",
    "        self.mlp = MLP(args)\n",
    "        self.norm =  nn.LayerNorm(args.n_embed)\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm(x))\n",
    "        return x + self.mlp(self.norm(x))\n",
    "    \n",
    "class GPT(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(args.vocab_size, args.n_embed),\n",
    "            wpe = nn.Embedding(args.block_size, args.n_embed),\n",
    "            drop=nn.Dropout(args.dropout),\n",
    "            h = nn.ModuleList([Block(args) for _ in range(args.n_layer)]),\n",
    "            norm = nn.LayerNorm(args.n_embed)\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(args.n_embed, args.vocab_size, bias=False)\n",
    "        self.param_nums = 0\n",
    "        # 初始化\n",
    "        self.lm_head.weight = self.transformer.wte.weight\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        for pname, p in self.named_parameters():\n",
    "            self.param_nums += p.numel()\n",
    "            if pname.endswith('attn_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0, std=0.02/math.sqrt(1*args.n_layer))\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module ,nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0,std=0.02)\n",
    "            if module.bias is not None:\n",
    "                 torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0,std=0.02)\n",
    "            \n",
    "    def forward(self, idx, target=None):\n",
    "        B,T = idx.shape\n",
    "        device = idx.device\n",
    "        pos = torch.arange(0,T,dtype=torch.long,device=device)\n",
    "        \n",
    "        embed_wte = self.transformer.wte(idx)\n",
    "        embed_wpe = self.transformer.wpe(pos)\n",
    "        x = self.transformer.dropout(embed_wte + embed_wpe)\n",
    "        \n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.norm(x)\n",
    "        \n",
    "        if target is not None:\n",
    "            logits = self.lm_head(x) # logits [B,T,vocab_size]\n",
    "            loss = F.cross_entropy(logits.reshape(-1,logits.shape[-1]), target.reshape(-1), ignore_index=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(x) # logits [B,T,vocab_size]\n",
    "            loss = None\n",
    "        \n",
    "        for name, p in model.named_parameters():\n",
    "            print('*'*100)\n",
    "            print(name, '/', p.shape)\n",
    "            print(p.requires_grad ,'/', p.grad)\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    \n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx = idx if idx.shape[-1] < self.args.block_size else idx[:, -self.args.block_size:] # 截断取最靠后的idx\n",
    "            logits, _ = self(idx) # logits[b,t,vocab_size]\n",
    "            logits = logits[:, -1, :] / temperature # logits[b,vocab_size]\n",
    "            \n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, k=top_k) # v [b,top_k]\n",
    "                logits[logits < v[:, [-1]]] = float('-inf')\n",
    "                \n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, next_token], dim=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T06:38:01.933416Z",
     "start_time": "2024-06-21T06:38:01.893289Z"
    }
   },
   "id": "ea19b8e2bf832096",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = GPT(args)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T06:38:03.401681Z",
     "start_time": "2024-06-21T06:38:03.135887Z"
    }
   },
   "id": "4cd3605bcad60fdd",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "optim = torch.optim.AdamW(model.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T06:38:25.593523Z",
     "start_time": "2024-06-21T06:38:24.565532Z"
    }
   },
   "id": "dab356e909922f8d",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[{'params': [Parameter containing:\n   tensor([[ 1.0698e-02, -4.9127e-03,  5.3695e-03,  ..., -2.4461e-02,\n            -1.1465e-02, -1.2741e-02],\n           [ 2.9380e-02, -3.7819e-02,  1.0553e-02,  ...,  2.6756e-03,\n             5.0633e-03, -2.0953e-02],\n           [-2.2765e-02, -3.0707e-02, -2.0424e-03,  ..., -1.5170e-02,\n            -6.7438e-03,  1.9553e-02],\n           ...,\n           [ 3.1154e-02, -7.2931e-05, -5.7943e-03,  ..., -3.1537e-02,\n            -2.2255e-02,  4.2320e-02],\n           [ 7.2942e-03, -2.8758e-02,  1.8473e-02,  ..., -6.4733e-03,\n             2.0547e-02, -2.9996e-02],\n           [ 4.0273e-02,  4.3315e-02,  1.0132e-02,  ..., -5.0604e-03,\n            -2.1345e-02, -1.4830e-02]], requires_grad=True),\n   Parameter containing:\n   tensor([[ 0.0440,  0.0336,  0.0088,  ...,  0.0151,  0.0057,  0.0012],\n           [-0.0339, -0.0020,  0.0474,  ...,  0.0140, -0.0073,  0.0079],\n           [ 0.0065,  0.0028, -0.0087,  ...,  0.0086,  0.0014, -0.0075],\n           ...,\n           [ 0.0192, -0.0170, -0.0052,  ...,  0.0029, -0.0319, -0.0180],\n           [ 0.0243,  0.0064, -0.0218,  ...,  0.0079,  0.0087, -0.0133],\n           [ 0.0009,  0.0295,  0.0225,  ..., -0.0078,  0.0128,  0.0055]],\n          requires_grad=True),\n   Parameter containing:\n   tensor([[-0.0108,  0.0162,  0.0074,  ..., -0.0169,  0.0181,  0.0065],\n           [-0.0745,  0.0076,  0.0168,  ...,  0.0133, -0.0017,  0.0207],\n           [-0.0131,  0.0099, -0.0182,  ..., -0.0146,  0.0019, -0.0163],\n           ...,\n           [-0.0152, -0.0215,  0.0161,  ..., -0.0093, -0.0253,  0.0064],\n           [-0.0028,  0.0231, -0.0122,  ...,  0.0001, -0.0051, -0.0024],\n           [-0.0308, -0.0232,  0.0173,  ...,  0.0049, -0.0006,  0.0131]],\n          requires_grad=True),\n   Parameter containing:\n   tensor([[ 0.0090,  0.0042,  0.0024,  ...,  0.0112,  0.0129,  0.0040],\n           [ 0.0314, -0.0125, -0.0089,  ..., -0.0072, -0.0109, -0.0028],\n           [-0.0138,  0.0232, -0.0113,  ..., -0.0045,  0.0134,  0.0134],\n           ...,\n           [ 0.0012,  0.0002, -0.0052,  ..., -0.0008,  0.0088, -0.0085],\n           [ 0.0006,  0.0144, -0.0132,  ..., -0.0032,  0.0032,  0.0244],\n           [ 0.0154,  0.0108,  0.0168,  ..., -0.0223,  0.0035,  0.0003]],\n          requires_grad=True),\n   Parameter containing:\n   tensor([[-6.8963e-03, -3.8092e-02,  8.2089e-03,  ...,  8.7376e-03,\n             3.6898e-03, -1.8411e-02],\n           [ 1.6427e-02,  5.0044e-05, -4.4976e-02,  ..., -1.7587e-02,\n             9.6203e-03, -1.0042e-02],\n           [-7.0419e-03,  3.1106e-02, -2.8841e-02,  ...,  3.3302e-03,\n            -2.9381e-03, -7.0256e-03],\n           ...,\n           [ 3.2081e-03, -1.1036e-02, -5.1015e-02,  ..., -2.8192e-03,\n            -7.4384e-03, -4.0421e-02],\n           [-1.1093e-02,  4.5529e-02, -2.1835e-03,  ..., -1.6925e-02,\n            -2.8251e-02,  9.4264e-03],\n           [ 1.0164e-02,  2.2417e-02,  1.3534e-02,  ..., -1.8383e-02,\n             1.6828e-02, -5.6639e-03]], requires_grad=True),\n   Parameter containing:\n   tensor([[-0.0117, -0.0261, -0.0238,  ...,  0.0192, -0.0036, -0.0454],\n           [-0.0186,  0.0195, -0.0257,  ...,  0.0100, -0.0030,  0.0106],\n           [-0.0363,  0.0090, -0.0265,  ...,  0.0025,  0.0080,  0.0126],\n           ...,\n           [ 0.0080, -0.0037, -0.0106,  ...,  0.0158, -0.0189,  0.0349],\n           [ 0.0365,  0.0186,  0.0169,  ...,  0.0042, -0.0356, -0.0362],\n           [ 0.0139,  0.0158, -0.0024,  ..., -0.0105, -0.0091,  0.0076]],\n          requires_grad=True),\n   Parameter containing:\n   tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n   Parameter containing:\n   tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n          requires_grad=True),\n   Parameter containing:\n   tensor([[-0.0055, -0.0122, -0.0024,  ..., -0.0158,  0.0066,  0.0141],\n           [ 0.0198,  0.0122,  0.0321,  ..., -0.0188, -0.0119,  0.0095],\n           [ 0.0149,  0.0119,  0.0218,  ...,  0.0307, -0.0052, -0.0230],\n           ...,\n           [-0.0143, -0.0038, -0.0345,  ..., -0.0121,  0.0144,  0.0219],\n           [-0.0001,  0.0208, -0.0521,  ...,  0.0302, -0.0167,  0.0069],\n           [ 0.0145,  0.0189,  0.0222,  ..., -0.0067,  0.0008, -0.0174]],\n          requires_grad=True),\n   Parameter containing:\n   tensor([[ 0.0031, -0.0029,  0.0332,  ..., -0.0078, -0.0022,  0.0030],\n           [ 0.0012,  0.0076, -0.0048,  ..., -0.0025,  0.0072,  0.0021],\n           [-0.0064, -0.0069,  0.0168,  ..., -0.0185, -0.0106,  0.0017],\n           ...,\n           [ 0.0030, -0.0193,  0.0084,  ..., -0.0194,  0.0241,  0.0187],\n           [ 0.0038,  0.0033, -0.0297,  ...,  0.0122, -0.0049, -0.0005],\n           [-0.0018, -0.0111,  0.0115,  ..., -0.0170, -0.0012,  0.0130]],\n          requires_grad=True),\n   Parameter containing:\n   tensor([[ 0.0139, -0.0327, -0.0121,  ...,  0.0107,  0.0208, -0.0064],\n           [-0.0265, -0.0103, -0.0311,  ...,  0.0046,  0.0334,  0.0081],\n           [ 0.0119,  0.0281, -0.0025,  ...,  0.0080, -0.0073,  0.0104],\n           ...,\n           [ 0.0067,  0.0050,  0.0062,  ...,  0.0003,  0.0488, -0.0046],\n           [ 0.0025,  0.0141,  0.0022,  ...,  0.0330, -0.0106,  0.0119],\n           [ 0.0286, -0.0194,  0.0043,  ...,  0.0676, -0.0249,  0.0046]],\n          requires_grad=True),\n   Parameter containing:\n   tensor([[ 0.0128,  0.0098,  0.0369,  ..., -0.0379,  0.0217,  0.0050],\n           [ 0.0235,  0.0238, -0.0292,  ...,  0.0050,  0.0235,  0.0091],\n           [ 0.0034,  0.0292, -0.0050,  ...,  0.0044, -0.0039, -0.0244],\n           ...,\n           [ 0.0304,  0.0157,  0.0018,  ...,  0.0272,  0.0023, -0.0272],\n           [ 0.0119, -0.0004, -0.0192,  ..., -0.0077, -0.0130, -0.0436],\n           [ 0.0231,  0.0128,  0.0006,  ..., -0.0280, -0.0431, -0.0237]],\n          requires_grad=True),\n   Parameter containing:\n   tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n   Parameter containing:\n   tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n          requires_grad=True),\n   Parameter containing:\n   tensor([[-0.0441, -0.0056,  0.0113,  ..., -0.0058,  0.0360,  0.0264],\n           [ 0.0054,  0.0002,  0.0130,  ...,  0.0095, -0.0201, -0.0132],\n           [ 0.0015,  0.0119,  0.0176,  ...,  0.0299,  0.0085, -0.0303],\n           ...,\n           [-0.0073, -0.0229,  0.0109,  ...,  0.0005, -0.0194,  0.0169],\n           [ 0.0087,  0.0033,  0.0065,  ...,  0.0259, -0.0264,  0.0251],\n           [ 0.0261, -0.0250,  0.0084,  ..., -0.0425,  0.0007, -0.0151]],\n          requires_grad=True),\n   Parameter containing:\n   tensor([[ 0.0031, -0.0156,  0.0026,  ..., -0.0054,  0.0142,  0.0106],\n           [ 0.0049, -0.0037, -0.0078,  ...,  0.0042, -0.0076,  0.0138],\n           [ 0.0020,  0.0028,  0.0219,  ...,  0.0060,  0.0105,  0.0072],\n           ...,\n           [-0.0136,  0.0044,  0.0040,  ..., -0.0025,  0.0126,  0.0215],\n           [-0.0192, -0.0048,  0.0153,  ..., -0.0128, -0.0160, -0.0335],\n           [-0.0038, -0.0195,  0.0033,  ..., -0.0077,  0.0110, -0.0163]],\n          requires_grad=True),\n   Parameter containing:\n   tensor([[-0.0074, -0.0372,  0.0091,  ...,  0.0034,  0.0139, -0.0153],\n           [-0.0401,  0.0039,  0.0224,  ...,  0.0291, -0.0149, -0.0160],\n           [ 0.0178, -0.0185,  0.0183,  ..., -0.0099,  0.0187, -0.0184],\n           ...,\n           [-0.0064, -0.0183, -0.0076,  ..., -0.0270,  0.0120,  0.0135],\n           [ 0.0088,  0.0373,  0.0230,  ...,  0.0083,  0.0219,  0.0094],\n           [ 0.0129, -0.0063,  0.0019,  ...,  0.0224, -0.0071,  0.0213]],\n          requires_grad=True),\n   Parameter containing:\n   tensor([[ 0.0174, -0.0073, -0.0113,  ..., -0.0032, -0.0080, -0.0332],\n           [-0.0141,  0.0012,  0.0088,  ..., -0.0351,  0.0442, -0.0345],\n           [ 0.0334,  0.0117, -0.0015,  ...,  0.0233,  0.0243, -0.0079],\n           ...,\n           [ 0.0032,  0.0270,  0.0222,  ..., -0.0078, -0.0251,  0.0148],\n           [-0.0312,  0.0162,  0.0141,  ...,  0.0191, -0.0114,  0.0148],\n           [ 0.0137, -0.0124, -0.0212,  ...,  0.0141, -0.0057,  0.0208]],\n          requires_grad=True),\n   Parameter containing:\n   tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n   Parameter containing:\n   tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n          requires_grad=True),\n   Parameter containing:\n   tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n   Parameter containing:\n   tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n          requires_grad=True)],\n  'lr': 0.3,\n  'betas': (0.9, 0.999),\n  'eps': 1e-08,\n  'weight_decay': 0.01,\n  'amsgrad': False,\n  'foreach': None,\n  'maximize': False,\n  'capturable': False,\n  'differentiable': False,\n  'fused': None}]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "optim.param_groups"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T06:42:24.727869Z",
     "start_time": "2024-06-21T06:42:24.688849Z"
    }
   },
   "id": "7cee63e0dbce1b83",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "transformer.wte.weight / torch.Size([7000, 768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.wpe.weight / torch.Size([128, 768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.0.attn.qkv.weight / torch.Size([2304, 768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.0.attn.attn_proj.weight / torch.Size([768, 768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.0.mlp.up_proj.weight / torch.Size([3072, 768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.0.mlp.down_proj.weight / torch.Size([768, 3072])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.0.norm.weight / torch.Size([768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.0.norm.bias / torch.Size([768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.1.attn.qkv.weight / torch.Size([2304, 768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.1.attn.attn_proj.weight / torch.Size([768, 768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.1.mlp.up_proj.weight / torch.Size([3072, 768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.1.mlp.down_proj.weight / torch.Size([768, 3072])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.1.norm.weight / torch.Size([768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.1.norm.bias / torch.Size([768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.2.attn.qkv.weight / torch.Size([2304, 768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.2.attn.attn_proj.weight / torch.Size([768, 768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.2.mlp.up_proj.weight / torch.Size([3072, 768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.2.mlp.down_proj.weight / torch.Size([768, 3072])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.2.norm.weight / torch.Size([768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.h.2.norm.bias / torch.Size([768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.norm.weight / torch.Size([768])\n",
      "True / None\n",
      "****************************************************************************************************\n",
      "transformer.norm.bias / torch.Size([768])\n",
      "True / None\n"
     ]
    }
   ],
   "source": [
    "for name,p in model.named_parameters():\n",
    "    print('*'*100)\n",
    "    print(name, '/', p.shape)\n",
    "    print(p.requires_grad ,'/', p.grad)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-20T05:22:29.451719Z",
     "start_time": "2024-06-20T05:22:29.438450Z"
    }
   },
   "id": "ec15c099194b86d1",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "63161100ed647433"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
